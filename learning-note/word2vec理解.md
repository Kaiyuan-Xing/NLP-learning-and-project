#  Word2vec的理解

自然语言处理（Natural Language Processing）是研究实现人与计算机之间有效通信的理论与方法的学科，通过一些有效的计算方法对各类语言的各级语言单位包括（字词句等）进行转换、存储、传输、分析，实现语音识别、文本分类、情感分析、机器翻译等多种语言问题。

### 词的表示方法

在NLP的各项任务中，我们需要将语言交给计算机来进行处理，但机器无法理解人类语言，因此首先要做的事情就是将语言进行数字化，然后再交给机器进行处理。而语言都是由词构成的，对于语言的数字化，其重点在于词的表示方法。以往的词表示方法常用两种：

##### （1）one-hot编码

one-hot编码，即独热编码，采用0和1的方式，对不同的特征状态，当该状态出现时为1，其他状态为0，并且在任意时刻，都只有一个状态为1。举例，假设有四个样本，每个样本有3个特征，特征1有两个状态，特征2有3个状态，特征3有四个状态，如图所示：

|       | 特征1 | 特征2 | 特征3 |
| :---: | :---: | :---: | :---: |
| 样本1 |   1   |   1   |   2   |
| 样本2 |   2   |   2   |   3   |
| 样本3 |   1   |   3   |   4   |
| 样本4 |   1   |   2   |   1   |

将上述样本特征转化为one-hot编码，则变为：

|       | 特征**1**-1 | 特征1-2 | 特征2-1 | 特征2-2 | 特征2-3 | 特征3-1 | 特征3-2 | 特征3-3 | 特征3-4 |
| ----- | ----------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |
| 样本1 | 1           | 0       | 1       | 0       | 0       | 0       | 1       | 0       | 0       |
| 样本2 | 0           | 1       | 0       | 1       | 0       | 0       | 0       | 1       | 0       |
| 样本3 | 1           | 0       | 0       | 0       | 1       | 0       | 0       | 0       | 1       |
| 样本4 | 1           | 0       | 0       | 1       | 0       | 1       | 0       | 0       | 0       |

从图中可以看书，采用one-hot编码后，特征维度增加，特征矩阵也变得更为稀疏。而将词采用这种方法表示时，先建立词典，按照词在词典中的位置，将词表示为词典长度的向量，1的位置对应该词在词典中的索引，其它则全为0。

 		采用one-hot方法进行词表示，虽然简单，可快速实现数字化，但存在较大缺点：1、它是一种词袋模型，不考虑顺序问题，而实际单词的出现是有顺序的；2、这种表示方法是假设词与词之间的独立的，忽略了语义和语境的问题，无法表达词语之间的相互关系；3、维度过高且稀疏，词表示维度由词典长度决定，而通常词典长度都很大，导致词表示维度也会非常大。

##### （2）Distributed representation

Distribute representation，即分布式表示，这种方法是通过训练，将每个词都映射到一个向量空间中，用一个长度较短且固定的向量来表示，将其视作向量空间中的点，从而可以根据空间距离来判断词之间的相似性。举个例子，单词king和queen，单词man和woman，通过训练得到他们的向量，并采用降维的方法，将其降到2维，便可以得到：

$$\underset{King}{\rightarrow} - \underset{Man}{\rightarrow} + \underset{Woman}{\rightarrow} = \underset{Queen}{\rightarrow}$$

![](C:\Users\邢开源\Desktop\king-queen.png)



这表明，词的上下文信息可以使词向量带有语义信。

### Word2vec

如何获得词向量呢？

 **word2vec**，一种学习词向量的框架，是一种神经网络概率语言模型，他训练的大致步骤如下：

* 构建包含大量文本语料的词库，并按照词出现顺序构造词典
* 将文本中的词由一个词向量来表示，可以先用one-hot方式
* 获取文本中词的位置，并将其作为中心词W，获取其上下文Context，即前后n个词
* 将中心词作为输入，通过某种映射关系，来预测上下文，输出上下文出现的概率，或者将上下文作为输入，预测中心词的概率，前者为**Skip-gram**模型，后者为**CBOW**模型，在CS224n的视频中，未提及这两种模型。
* 不断优化训练参数，使得p（w|c）或p（c|w）最大。

p（w|c）或P(c|w)的计算采用softmax函数，可得：

$$p(w|context(w)) =\frac {exp(w0^TVc)}{\sum_{i=1}^{N}exp(wi^TVc)} $$

word2vec在训练过程中，为获得最大概率，需要定义损失函数 ：

$$ J（\theta）=-\frac 1TlogL(\theta)=-\frac1T\sum_{t=1}^T\sum_{-m\le j\le m \ \ \ \ \ j\ne 0}logP(w_{t+j}|w_t;\theta) $$

注：此损失函数是用对数似然函数，为Manning教授推导所使用的函数，但在CBOW模型和Skip-gram模型中常用的的损失函数是采用负采样的方法后在此函数基础上进行转换，采用交叉熵损失函数，即：

$$\sum_{w\epsilon C}\sum_{u\epsilon {w}\cup NEG(w)}{L^w(u)\cdot log\left [\sigma(x_w^T\theta ^u)  \right ] + \left [ 1-L^w(u) \right ]\cdot log\left [\sigma(x_w^T\theta ^u) \right ] }$$

具体的推导过程详细见这篇[博客](https://www.cnblogs.com/peghoty/p/3857839.html)，这里不在阐述。

构造完损失函数以后，下一步需要做的就是进行最小化损失了，通常采用梯度下降算法来进行参数更新。训练完毕后，可以获得一个参数矩阵W，通过输入词向量与矩阵相乘从而得到我们想要的词向量，因此参数矩阵W也可以称为查找表，词表中的任何一个词通过该矩阵都能得到自己的词向量。

关于**word2vec**中的CBOW模型和Skip-gram模型，Manning教授虽然未做阐述，但也需要进一步了解。

#### CBOW模型

CBOW模型是一种神经网络模型，是将某个中心词的上下文，即前n个词或者前后n个词来作为输入，来预测中心词。

![](E:\AI工程师NLP方向\CS224n\lession1\cbow.jpg)

如图所示，该模型一般由输入层、隐藏层和输出层的三层网络结构组成：

* 输入层：一般是上下文单词的one-hot向量，单词向量空间的维度为V，即整个词表的大小为V，上下文窗口的大小为C
* 假设最终词向量的维度为N，参数矩阵W的大小即为V*N，训练时可对W进行初始化
* 输出权重矩阵为W‘，大小为N*V
* 前向传播时，将输入向量与权重矩阵W相乘，得到隐藏层1*N的向量，由于有C个输入，对C个1N的矩阵取平均值，得到一个1N的向量
* 将得到1*N的向量与W’相乘，并采用softmax函数处理，得到1V得向量，此时每一维对应词表中的单词，得到概率最大的索引对应的单词即为所预测的中心词
* 之后，计算损失函数，通常采用交叉熵损失函数，进行反向传播来更新W和W'，来最小化损失函数

#### Skip-gram模型

与CBOW模型一样，Skip-gram模型也是一种神经网络模型，但不同的是Skip-gram模型是用中心词来预测上下文。

![](E:\AI工程师NLP方向\CS224n\lession1\skip-gram.png)

如图所示，其结构与CBOW一样，由三层网络组成，不同的地方在于输入向量和输出向量的个数，其输入向量为中心词的one-hot向量，输出向量为上下文词出现的概率结果是多个词，训练过程与CBOW基本一样，不在赘述。

### Word2vec的训练方法

Word2vec在训练过程中，其模型参数由词表的长度确定，导致其权重矩阵较大，另外输出神经元个数也与词表长度有关，在计算loss时需要计算所有词的预测误差，若词表长度为10000，则要计算10000个词的误差，然后再进行反向传播更新参数，这将导致计算量过大，训练速度非常慢。

对于训练方法的改进，主要有以下两种：

#### （1）Hierarchical Softmax 

Hierarchical Softmax，即分层softmax函数，是将模型的输出由线性结构转化为一棵Huffman树（最优二叉树），叶子节点个数等于词表中词的个数，预测某个词的概率即转化为找到这个词所在分支的最优路径概率，从而将多分类问题变成了连续的二分类问题，如图所示：

![](E:\AI工程师NLP方向\CS224n\lession1\Hierarchical sotfmax.jpg)

详细的推导过程不做介绍，感兴趣的可以去看参考文献【1】。

#### （2）Negative Sampling

Negative Sampling，即负采样，即在训练过程中从词表中随机抽取未出现的词来作为负样本，计算预测样本误差，避免计算所有词的预测误差，降低了计算复杂度，提高了效率，同时也将多分类的问题转化为了近似二分类的问题，并且对目标函数也进行了调整：

$$-\sum_{w\epsilon C}\left \{log\left [\sigma(x_w^T\theta ^w)  \right ] + \sum_{w\epsilon NEG(w)}log\left [-\sigma(x_w^T\theta ^u) \right ] \right \}$$

具体的推导过程详见参考文献【1】。

在word2vec实际的训练过程中，采用负采样的方法可以快速实现，且效果显著，而分层softmax的实现较为复杂，且最终效果并没有明显优于负采样方法，因此使用更多的是负采样的训练方法。

### 总结

本文大概介绍了Word2vec的一些基本原理，对CS224n视频中教授并未讲到的cbow和skip-gram也有涉及，同时Word2vec作为NLP领域的重要组成部分，需要有进一步的理解。

#### 参考文献

1、[word2vec中的数学原理详解.](https://www.cnblogs.com/peghoty/p/3857839.html)

2、[通俗理解word2vec.](https://www.jianshu.com/p/471d9bfbd72f)

